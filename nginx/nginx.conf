# ======================================================================
# NGINX — reverse proxy + balanceador simples para duas instâncias Node
# Stack esperada (docker-compose):
#   - app1:3001
#   - app2:3002
#   - nginx escutando na porta 9999 (exposta no host)
#
# DICAS RÁPIDAS
#   Testar sintaxe:           nginx -t
#   Ver versão/compilação:    nginx -V
#   Logs (se habilitar):      /var/log/nginx/access.log e error.log
#
# NOTAS IMPORTANTES
# - Este arquivo assume Linux (usa epoll).
# - 'proxy_buffering off' é ótimo p/ baixa latência e streaming,
#   mas reduz throughput e aumenta carga no upstream.
# - 'keepalive' no 'upstream' mantém conexões HTTP/1.1 abertas
#   com os apps (menos handshakes, mais performance).
# ======================================================================

worker_processes auto;                # Uma worker por CPU lógica (NGINX decide automaticamente).
                                      # Se você tiver muita I/O bloqueante, 'auto' costuma ser a melhor escolha.

events {
    worker_connections 8192;          # Máximo de conexões *por worker*. Com 4 workers: ~32k conexões potenciais.
                                      # Lembre-se: cada conexão cliente pode gerar outra com o upstream (proxy).

    multi_accept on;                  # Ao receber o evento de "há conexões", aceita quantas couberem no socket.
                                      # Acelera picos de conexão, mas pode aumentar a carga instantânea.

    use epoll;                        # Mecanismo de I/O escalável no Linux (edge-triggered).
                                      # Para BSD/macOS, seria 'kqueue'; em Windows, 'select'/'IOCP' (dependendo do build).
}

http {
    access_log off;                   # Desliga access log (menos IO). Útil em benchmarks.
                                      # PRODUÇÃO: considere habilitar para auditoria e diagnóstico.

    sendfile on;                      # Envia arquivos diretamente do kernel (zero-copy). Bom para estáticos e proxy.
    tcp_nopush on;                    # Agrupa cabeçalhos/segmentos (com sendfile) para reduzir pacotes; útil em respostas grandes.
    tcp_nodelay on;                   # Não espere por mais dados para enviar (baixa latência para respostas pequenas).

    keepalive_timeout 65s;            # Tempo que o NGINX mantém conexões *com o cliente* abertas sem tráfego.
                                      # Muito curto => mais handshakes; muito longo => consome FDs.

    keepalive_requests 10000;         # Quantidade máxima de requisições por conexão persistente do *cliente*.
                                      # Alto = menos handshakes; se notar conexões muito antigas, reduza.

    # ------------------------------------------------------------------
    # Grupo de upstreams "api" — onde o NGINX vai distribuir o tráfego.
    # Sem 'least_conn' ou 'ip_hash', o algoritmo default é round-robin.
    # 'keepalive 300' mantém até 300 conexões IDLE por worker com o upstream.
    # ------------------------------------------------------------------
    upstream api {
        server app1:3001;             # Backend 1 (nome do serviço Docker). Sem peso explícito => peso 1.
        server app2:3002;             # Backend 2. Se quiser favorecer um nó: 'server app2:3002 weight=2;'

        keepalive 300;                # Pool de conexões persistentes HTTP/1.1 com o upstream.
                                      # Benefícios: menos handshakes TCP/TLS, menor latência, mais RPS.
                                      # Requer 'proxy_http_version 1.1' e 'Connection: ""' (sem 'close').
    }

    server {
        listen 9999;                  # Porta do NGINX que o host expõe (deve bater com 'ports: "9999:9999"' no Compose).

        location / {
            proxy_http_version 1.1;   # HTTP/1.1 necessário para keep-alive com upstream e para websockets (com outras flags).
                                       # HTTP/1.0 não mantém persistência por padrão.

            proxy_set_header Connection "";   # Remove cabeçalho hop-by-hop 'Connection' do request enviado ao upstream.
                                              # Evita 'Connection: close' acidental; permite reutilização no pool keepalive.

            proxy_buffering off;      # Entrega streaming e baixa latência (responde ao cliente conforme chega do upstream).
                                      # CUSTO: menos throughput agregado e maior uso de CPU no upstream.
                                      # Se seu caso é alta taxa de respostas pequenas, testar 'on' pode melhorar QPS.

            proxy_pass http://api;    # Encaminha para o 'upstream api' (round-robin entre app1 e app2).
                                      # Em caso de 502/504 frequentes, investigar timeouts no upstream
                                      # (Node.js keepAliveTimeout, limites de pool do banco, etc.).
        }
    }
}
